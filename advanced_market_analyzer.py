import os
import json
import feedparser
import yfinance as yf
from datetime import datetime, timedelta
from notion_client import Client, APIResponseError
from dotenv import load_dotenv
import google.generativeai as genai
import time

# --- 1. ì„¤ì • ë° ì´ˆê¸°í™” ---
print("=" * 60)
print("ê³ ê¸‰ ì‹œì¥ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
print("=" * 60)

load_dotenv()

# API í‚¤ ë° ID ë¡œë“œ
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
NOTION_FEEDBACK_DB_ID = os.getenv("NOTION_FEEDBACK_DB_ID")
NOTION_REPORT_DB_ID = os.getenv("NOTION_REPORT_DB_ID")

# API í˜¸ì¶œ íšŸìˆ˜ ì¹´ìš´í„°
api_call_counter = {'gemini': 0, 'notion': 0}

# ìœ íš¨ì„± ê²€ì‚¬
if not all([NOTION_API_KEY, GEMINI_API_KEY, NOTION_DATABASE_ID, NOTION_FEEDBACK_DB_ID, NOTION_REPORT_DB_ID]):
    raise ValueError("í•˜ë‚˜ ì´ìƒì˜ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
notion = Client(auth=NOTION_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel('gemini-pro-latest')

# RSS í”¼ë“œ ì†ŒìŠ¤
RSS_FEEDS = {
    "Yahoo Finance": "https://finance.yahoo.com/rss/topstories",
    "CNBC": "https://www.cnbc.com/id/100003114/device/rss/rss.html",
    "MarketWatch": "http://feeds.marketwatch.com/marketwatch/topstories/",
    "Seeking Alpha": "https://seekingalpha.com/feed.xml"
}

# --- 2. í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
def get_batch_analysis_prompt(articles):
    article_inputs = []
    for i, article in enumerate(articles):
        article_inputs.append(f"<article index=\"{i}\"><title>{article['title']}</title><content>{article['summary']}</content></article>")
    
    return f'''
    You are a senior equity analyst at a top-tier investment firm. Your job is to analyze news and predict short-term stock price movements with high accuracy.

    ### News Articles:
    {''.join(article_inputs)}

    ### Analysis Framework:
    For each article, perform this structured analysis:

    1. **Materiality Check**: Does this news contain concrete, actionable information? Skip generic/rehashed content.

    2. **Causal Analysis**:
        - What specific event occurred? (earnings beat/miss, product launch, etc.)
        - What is the magnitude? (revenue numbers, user growth %)
        - What is the market expectation vs. reality gap?

    3. **Pre-Mortem Analysis (CRITICAL STEP)**:
        - Form an initial prediction (Positive/Negative).
        - **Then, explicitly state the top 3 strongest arguments or risk factors for why your initial prediction might be WRONG.** This analysis will populate the `pre_mortem_risks` field.

    4. **Price Impact Logic & Final Sentiment**:
        - Bullish: Clear, positive catalyst (revenue beat, strategic win).
        - Bearish: Clear, negative catalyst (guidance cut, regulatory headwind).
        - **Neutral: [NEW RULE] Classify as 'Neutral' if the predicted change is marginal (e.g., within a +/- 0.5% range), if signals are mixed (e.g., revenue beat but guidance miss), or if there is no clear catalyst.**

    5. **Confidence Calibration**:
        - Based on your Pre-Mortem. If `pre_mortem_risks` are strong, confidence must be lower.
        - High (8-10): Clear catalyst + quantifiable impact + low-risk pre-mortem.
        - Medium (5-7): Important but lacks specifics, or pre-mortem identifies significant counter-arguments.
        - Low (1-4): Speculative, opinion-based, or high-risk pre-mortem.

    ### Output Format (JSON):
    {{
      "article_index": <int>,
      "korean_title": "ê¸°ì‚¬ ì œëª©ì„ í•œê¸€ë¡œ ë²ˆì—­",
      "mentioned_tickers": ["AAPL", "MSFT"],
      "sentiment": "Positive|Negative|Neutral",
      "conviction_score": <1-10>,
      "summary": "í•œê¸€ë¡œ 3ë¬¸ì¥ ë¶„ì„:
        1) í•µì‹¬ ì‚¬ê±´: [êµ¬ì²´ì  ìˆ˜ì¹˜/ì‚¬ì‹¤ í¬í•¨]
        2) ì£¼ê°€ ì˜í–¥ ë…¼ë¦¬: [ì™œ ì˜¤ë¥´ê±°ë‚˜ ë‚´ë¦´ ê²ƒì¸ê°€]
        3) ì‹œê°„ í”„ë ˆì„: [ë‹¨ê¸°/ì¤‘ê¸° ì˜í–¥]",
      "pre_mortem_risks": "KOREAN: ì´ ì˜ˆì¸¡ì´ í‹€ë¦´ ìˆ˜ ìˆëŠ” ê°€ì¥ ê°•ë ¥í•œ ì´ìœ  3ê°€ì§€ (Pre-mortem ë¶„ì„ ê²°ê³¼)"
    }}

    ### Quality Standards:
    - âŒ Bad: "ì• í”Œì˜ ì‹ ì œí’ˆ ì¶œì‹œë¡œ ê¸ì •ì  ì „ë§"
    - âœ… Good (Example Output):
      {{
        "article_index": 0,
        "korean_title": "ì• í”Œ, ì•„ì´í°15 ì‚¬ì „ì˜ˆì•½ ì „ë…„ ëŒ€ë¹„ 20% ì¦ê°€",
        "mentioned_tickers": ["AAPL"],
        "sentiment": "Positive",
        "conviction_score": 7,
        "summary": "1) í•µì‹¬ ì‚¬ê±´: ì•„ì´í°15 ì‚¬ì „ì˜ˆì•½ì´ ì „ë…„ ëŒ€ë¹„ 20% ì¦ê°€í•˜ë©° ì´ˆê¸° ìˆ˜ìš” ê°•ì„¸ í™•ì¸.\n2) ì£¼ê°€ ì˜í–¥ ë…¼ë¦¬: ë‹¨ê¸° ë§¤ì¶œ ê¸°ëŒ€ê° ìƒìŠ¹ìœ¼ë¡œ ì£¼ê°€ì— ê¸ì •ì .\n3) ì‹œê°„ í”„ë ˆì„: ë‹¨ê¸° (ë‹¤ìŒ ë¶„ê¸° ì‹¤ì  ë°œí‘œ ì „)",
        "pre_mortem_risks": "1) ê±°ì‹œê²½ì œ ìœ„ì¶•ìœ¼ë¡œ ì‹¤ì œ íŒë§¤ ì „í™˜ìœ¨ì´ ë‚®ì„ ìˆ˜ ìˆìŒ.\n2) ë†’ì€ ë¶€í’ˆ ë¹„ìš©ìœ¼ë¡œ ì¸í•´ ë§ˆì§„ì´ ì••ë°•ë°›ì„ ìˆ˜ ìˆìŒ.\n3) í•´ë‹¹ ë‰´ìŠ¤ê°€ ì´ë¯¸ ì‹œì¥ ê¸°ëŒ€ì¹˜ì— ì„ ë°˜ì˜ë˜ì—ˆì„ ê°€ëŠ¥ì„±."
      }}

    ### Critical Rules:
    - Only analyze articles mentioning **specific publicly traded tickers**.
    - Distinguish between "company did X" (fact) vs. "analyst says X" (opinion).
    - **[NEW RULE] Your `conviction_score` MUST reflect your `pre_mortem_risks`. If `pre_mortem_risks` are significant, the score CANNOT be 9 or 10.**

    Return ONLY a valid JSON array. No explanations outside JSON.
    '''

def get_weekly_feedback_and_prompt_improvement_prompt(failed_predictions, successful_predictions):
    total = len(failed_predictions) + len(successful_predictions)
    accuracy = (len(successful_predictions) / total * 100) if total > 0 else 0

    return f'''
    You are an AI performance auditor. Analyze prediction accuracy and suggest improvements.

    ### Performance Data:
    - **Failed Predictions (with pre-mortem)**: 
      - Each object shows the prediction that failed, and the `pre_mortem` analysis the AI wrote *before* it failed.
      - {str(failed_predictions)}
    - **Successful Predictions**: {str(successful_predictions)}
    - **Accuracy**: {accuracy:.1f}%

    ### Your Task:
    Analyze patterns and generate a JSON report. Only suggest improvements if you find **systematic, recurring failures**.

    **CRITICAL ANALYSIS for `failure_analysis`**:
    For each failure, check its `pre_mortem` field.
    1.  **"Ignored Risk"**: Did the AI correctly identify the risk in its `pre_mortem` but still make the wrong call (e.g., high conviction)?
    2.  **"Missed Risk"**: Was the failure caused by a risk the AI completely failed to identify in its `pre_mortem`?
    Your root cause analysis MUST differentiate between these two failure types.

    **JSON Format:**
    {{
      "weekly_summary": {{
        "total_predictions": {total},
        "correct_predictions": {len(successful_predictions)},
        "accuracy_rate": "{accuracy:.1f}%",
        "key_takeaway": "One-sentence summary in KOREAN"
      }},
      "failure_analysis": {{
        "recurring_theme": "What common mistake pattern exists? (e.g., 'Ignored Risk', 'Missed Risk') (KOREAN)",
        "examples": ["TSLA", "AAPL"],
        "root_cause": "Why did the AI fail? Did it ignore its own pre-mortem? (KOREAN)"
      }},
      "success_analysis": {{
        "common_pattern": "What makes successful predictions accurate? (KOREAN)",
        "examples": ["NVDA"]
      }},
      "actionable_improvement": {{
        "needed": true/false,
        "problem": "Core weakness if systematic failure exists (KOREAN)",
        "solution": "Specific prompt modification suggestion",
        "expected_impact": "How this will improve accuracy (KOREAN)"
      }}
    }}

    **Important**: Set "needed": false if accuracy > 70% or failures are random.
    Return ONLY the JSON object.
    '''

# --- 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def check_notion_connections():
    print("\n[ë‹¨ê³„ 1/6] Notion ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸ ì¤‘...")
    db_map = {
        "ë©”ì¸ ë¶„ì„ DB": NOTION_DATABASE_ID,
        "ì¼ì¼ í”¼ë“œë°± DB": NOTION_FEEDBACK_DB_ID,
        "ì£¼ê°„ ë³´ê³ ì„œ DB": NOTION_REPORT_DB_ID
    }
    all_connected = True
    for name, db_id in db_map.items():
        try:
            notion.databases.retrieve(database_id=db_id)
            api_call_counter['notion'] += 1
            print(f"  âœ“ {name}: ì—°ê²° ì„±ê³µ")
        except APIResponseError as e:
            print(f"  âœ— {name}: ì—°ê²° ì‹¤íŒ¨! .env íŒŒì¼ì˜ IDê°€ ì •í™•í•œì§€, Notionì—ì„œ í†µí•© ê¶Œí•œì„ ë¶€ì—¬í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            print(f"    ì˜¤ë¥˜: {e}")
            all_connected = False
    if not all_connected:
        raise ConnectionError("Notion ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    print("ëª¨ë“  Notion ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

def fetch_news_from_rss(feeds):
    print("\n[ë‹¨ê³„ 2/6] RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    articles = []
    for name, url in feeds.items():
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries[:10]:
                articles.append({
                    'source': name,
                    'title': entry.title,
                    'link': entry.link,
                    'summary': entry.get('summary', ''),
                    'published': entry.get('published', datetime.now().isoformat())
                })
            print(f"  âœ“ {name}: {len(feed.entries[:10])}ê°œ ìˆ˜ì§‘")
        except Exception as e:
            print(f"  âœ— {name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    print(f"ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return articles

def analyze_articles_in_batch(articles, batch_size=4):
    print("\n[ë‹¨ê³„ 3/6] Gemini ë°°ì¹˜ ë¶„ì„ ì‹œì‘ (ë¶„ë‹¹ 2íšŒ ì œí•œ ì¤€ìˆ˜)...")
    all_results = []
    for i in range(0, len(articles), batch_size):
        batch = articles[i:i+batch_size]
        print(f"  - ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ê¸°ì‚¬ ë¶„ì„ ìš”ì²­...")
        try:
            prompt = get_batch_analysis_prompt(batch)
            response = gemini_model.generate_content(prompt)
            api_call_counter['gemini'] += 1
            print("    - Gemini API í˜¸ì¶œ ì™„ë£Œ. 30ì´ˆ ëŒ€ê¸°...")
            time.sleep(30)

            json_text = response.text.strip().replace("```json", "").replace("```", "").strip()
            batch_results = json.loads(json_text)
            
            for result in batch_results:
                article_index = result.get("article_index")
                if article_index is not None and 0 <= article_index < len(batch):
                    result['original_article'] = batch[article_index]
            
            all_results.extend(batch_results)
        except Exception as e:
            print(f"  âœ— ë°°ì¹˜ {i//batch_size + 1} ë¶„ì„ ì‹¤íŒ¨: {e}")
    print(f"ì´ {len(all_results)}ê°œì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.")
    return all_results

def save_analysis_to_notion(analysis_results):
    print("\n[ë‹¨ê³„ 4/6] Notionì— ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
    count = 0
    for result in analysis_results:
        # í•„í„°ë§: ì¢…ëª©ì´ ì—†ê±°ë‚˜ í™•ì‹ ë„ê°€ 6 ë¯¸ë§Œì¸ ê²½ìš° ì œì™¸
        if not result.get("mentioned_tickers") or result.get("conviction_score", 0) < 6:
            continue

        article = result.get('original_article', {})

        # [!] "AI Pre-mortem" ì†ì„± ì¶”ê°€ (Notion DBì— í•´ë‹¹ ì†ì„±ì´ ìˆì–´ì•¼ í•¨)
        properties = {
            "ê¸°ì‚¬ ì œëª©": {"title": [{"text": {"content": result.get("korean_title", article.get('title', 'N/A'))}}]},
            "ì–¸ê¸‰ëœ ì¢…ëª©": {"rich_text": [{"text": {"content": ", ".join(result.get("mentioned_tickers", []))}}]},
            "ê°ì„±ë¶„ì„": {"select": {"name": result.get("sentiment", "Neutral")}},
            "AI í™•ì‹  ì ìˆ˜": {"number": result.get("conviction_score", 0)},
            "AI ë¶„ì„ ìš”ì•½": {"rich_text": [{"text": {"content": result.get("summary", "")}}]},
            "AI Pre-mortem": {"rich_text": [{"text": {"content": result.get("pre_mortem_risks", "")}}]} ,
            "URL": {"url": article.get('link', "")}
        }

        # í˜ì´ì§€ ë³¸ë¬¸ì— ì›ë¬¸ ìš”ì•½ ì¶”ê°€
        children = [
            {"object": "block", "type": "heading_2", "heading_2": {"rich_text": [{"text": {"content": "ğŸ“° ê¸°ì‚¬ ì›ë¬¸ ìš”ì•½"}}]}},
            {"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"text": {"content": article.get('summary', 'N/A')[:2000]}}]}}  # Notion 2000ì ì œí•œ
        ]

        try:
            notion.pages.create(parent={"database_id": NOTION_DATABASE_ID}, properties=properties, children=children)
            api_call_counter['notion'] += 1
            count += 1
            print(f"  âœ“ ì €ì¥: {result.get('korean_title', 'N/A')[:30]}... (í™•ì‹ ë„: {result.get('conviction_score')})")
        except APIResponseError as e:
            print(f"  âœ— Notion ì €ì¥ ì˜¤ë¥˜: {result.get('korean_title', '')} - {e}")

    print(f"âœ“ ì´ {count}ê°œì˜ ìœ ì˜ë¯¸í•œ ë¶„ì„ì„ Notionì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    
def run_daily_feedback_check():
    print("\n[ë‹¨ê³„ 5/6] ì¼ì¼ í”¼ë“œë°± ê²€ì¦ ì‹œì‘...")
    yesterday = (datetime.now() - timedelta(days=1)).isoformat()
    try:
        response = notion.databases.query(database_id=NOTION_DATABASE_ID, filter={"timestamp": "created_time", "created_time": {"on_or_after": yesterday}})
        api_call_counter['notion'] += 1
        predictions = response.get("results", [])
        if not predictions:
            print("  - ê²€ì¦í•  ì–´ì œ ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"  - {len(predictions)}ê°œì˜ ì–´ì œ ì˜ˆì¸¡ì„ ê²€ì¦í•©ë‹ˆë‹¤.")
        for page in predictions:
            try:
                props = page.get("properties", {})
                tickers_text = props.get("ì–¸ê¸‰ëœ ì¢…ëª©", {}).get("rich_text", [{}])[0].get("text", {}).get("content", "")
                predicted_sentiment = props.get("ê°ì„±ë¶„ì„", {}).get("select", {}).get("name")
                
                # [!] ì›ë³¸ Pre-mortem í…ìŠ¤íŠ¸ ì½ì–´ì˜¤ê¸°
                pre_mortem_text = props.get("AI Pre-mortem", {}).get("rich_text", [{}])[0].get("text", {}).get("content", "")
                
                if not tickers_text or predicted_sentiment not in ["Positive", "Negative"]:
                    continue

                ticker = tickers_text.split(",")[0].strip()
                stock = yf.Ticker(ticker)
                hist = stock.history(period="2d")
                
                if len(hist) < 2:
                    continue

                actual_change = (hist['Close'].iloc[-1] - hist['Close'].iloc[-2]) / hist['Close'].iloc[-2] * 100
                
                correct = (predicted_sentiment == "Positive" and actual_change > 0) or (predicted_sentiment == "Negative" and actual_change < 0)
                
                # [!] "Pre-mortem ì›ë³¸" ì†ì„± ì¶”ê°€ (í”¼ë“œë°± DBì— í•´ë‹¹ ì†ì„±ì´ ìˆì–´ì•¼ í•¨)
                feedback_properties = {
                    "ì¢…ëª©": {"title": [{"text": {"content": ticker}}]},
                    "ì˜ˆì¸¡ ë°©í–¥": {"select": {"name": predicted_sentiment}},
                    "ì‹¤ì œ ë³€ë™": {"number": round(actual_change, 2)},
                    "ì˜ˆì¸¡ ì •í™•": {"checkbox": bool(correct)}, # numpy.bool_ë¥¼ í‘œì¤€ boolë¡œ ë³€í™˜
                    "ì›ì¸ ë¶„ì„": {"rich_text": [{"text": {"content": "ì„±ê³µ: ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì›€ì§ì„ ì¼ì¹˜" if correct else "ì‹¤íŒ¨: ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì›€ì§ì„ ë¶ˆì¼ì¹˜"}}]},
                    "Pre-mortem ì›ë³¸": {"rich_text": [{"text": {"content": pre_mortem_text}}]} # [!] Pre-mortem ë°ì´í„° ë³µì‚¬
                }
                notion.pages.create(parent={"database_id": NOTION_FEEDBACK_DB_ID}, properties=feedback_properties)
                api_call_counter['notion'] += 1
                print(f"  âœ“ í”¼ë“œë°± ì €ì¥: {ticker} (ì˜ˆì¸¡: {predicted_sentiment}, ì‹¤ì œ: {actual_change:.2f}%) -> {'ì„±ê³µ' if correct else 'ì‹¤íŒ¨'}")
            except Exception as e:
                print(f"  âœ— í”¼ë“œë°± ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    except APIResponseError as e:
        print(f"âœ— ì–´ì œ ë¶„ì„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")

def run_weekly_report_generation():
    print("\n[ì¶”ê°€ ì‘ì—…] ì£¼ê°„ í”¼ë“œë°± ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
    last_week = (datetime.now() - timedelta(days=7)).isoformat()
    try:
        response = notion.databases.query(database_id=NOTION_FEEDBACK_DB_ID, filter={"timestamp": "created_time", "created_time": {"on_or_after": last_week}})
        api_call_counter['notion'] += 1
        feedback_logs = response.get("results", [])
        if not feedback_logs:
            print("  - ë¶„ì„í•  í”¼ë“œë°± ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        successful_predictions = []
        failed_predictions = []
        for page in feedback_logs:
            props = page.get("properties", {})
            prediction = {
                "ticker": props.get("ì¢…ëª©", {}).get("title", [{}])[0].get("text", {}).get("content", ""),
                "prediction": props.get("ì˜ˆì¸¡ ë°©í–¥", {}).get("select", {}).get("name"),
                "actual_change": props.get("ì‹¤ì œ ë³€ë™", {}).get("number"),
                "reason": props.get("ì›ì¸ ë¶„ì„", {}).get("rich_text", [{}])[0].get("text", {}).get("content", ""),
                "pre_mortem": props.get("Pre-mortem ì›ë³¸", {}).get("rich_text", [{}])[0].get("text", {}).get("content", "") # [!] Pre-mortem ë°ì´í„° ì¶”ê°€
            }
            if props.get("ì˜ˆì¸¡ ì •í™•", {}).get("checkbox"):
                successful_predictions.append(prediction)
            else:
                failed_predictions.append(prediction)
        
        print(f"  - ì§€ë‚œ ì£¼ ì˜ˆì¸¡ ê²°ê³¼: {len(successful_predictions)}ê°œ ì„±ê³µ, {len(failed_predictions)}ê°œ ì‹¤íŒ¨")
        
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¦¬í¬íŠ¸ ìƒì„± ìŠ¤í‚µ
        if not successful_predictions and not failed_predictions:
            print("  - ë¶„ì„í•  í”¼ë“œë°± ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¦¬í¬íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        prompt = get_weekly_feedback_and_prompt_improvement_prompt(failed_predictions, successful_predictions)
        response = gemini_model.generate_content(prompt)
        api_call_counter['gemini'] += 1
        print("    - Gemini API í˜¸ì¶œ ì™„ë£Œ. 30ì´ˆ ëŒ€ê¸°...")
        time.sleep(30)

        json_text = response.text.strip().replace("```json", "").replace("```", "").strip()
        report_data = json.loads(json_text)

        # ê°œì„ ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³´ê³ ì„œ ìƒì„±
        improvement = report_data.get("actionable_improvement", {})
        if improvement.get("needed"):
            print("  - ì‹œìŠ¤í…œì  ì‹¤íŒ¨ íŒ¨í„´ ë°œê²¬. ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

            report_title = f"ì£¼ê°„ í”¼ë“œë°± ë³´ê³ ì„œ ({datetime.now().strftime('%Yë…„ %mì›” %dì¼')})"
            summary = report_data.get("weekly_summary", {})
            failure = report_data.get("failure_analysis", {})
            success = report_data.get("success_analysis", {})

            properties = {
                "ë³´ê³ ì„œ ê¸°ê°„": {"title": [{"text": {"content": report_title}}]},
                "ì •í™•ë„": {"rich_text": [{"text": {"content": summary.get("accuracy_rate", "N/A")}}]},
                "í•µì‹¬ ìš”ì•½": {"rich_text": [{"text": {"content": summary.get("key_takeaway", "N/A")}}]},
                "ì‹¤íŒ¨ ì›ì¸ ë¶„ì„": {"rich_text": [{"text": {"content": f"í…Œë§ˆ: {failure.get('recurring_theme', 'N/A')}\nê·¼ë³¸ ì›ì¸: {failure.get('root_cause', 'N/A')}"}}]},
                "ì„±ê³µ ë¹„ê²° ë¶„ì„": {"rich_text": [{"text": {"content": success.get("common_pattern", "N/A")}}]},
                "ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì œì•ˆ": {"rich_text": [{"text": {"content": improvement.get("solution", "N/A")}}]}
            }
            notion.pages.create(parent={"database_id": NOTION_REPORT_DB_ID}, properties=properties)
            api_call_counter['notion'] += 1
            print(f"âœ“ ì£¼ê°„ í”¼ë“œë°± ë³´ê³ ì„œë¥¼ Notionì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("  - ì •í™•ë„ê°€ ì–‘í˜¸í•˜ê±°ë‚˜ ì‹¤íŒ¨ê°€ ë¬´ì‘ìœ„ì ì…ë‹ˆë‹¤. ë³´ê³ ì„œ ìƒì„±ì„ ìƒëµí•©ë‹ˆë‹¤.")

    except Exception as e:
        print(f"  âœ— ì£¼ê°„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")

# --- 9. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def main():
    try:
        check_notion_connections()
        
        # ì›”ìš”ì¼ì—ë§Œ ì£¼ê°„ ë³´ê³ ì„œ ìƒì„±
        if datetime.now().weekday() == 0:
            run_weekly_report_generation()

        # ë§¤ì¼ ì‹¤í–‰ë˜ëŠ” ë¶„ì„ ë° í”¼ë“œë°±
        run_daily_feedback_check()
        articles = fetch_news_from_rss(RSS_FEEDS)
        if articles:
            analysis_results = analyze_articles_in_batch(articles)
            if analysis_results:
                save_analysis_to_notion(analysis_results)
        
    except Exception as e:
        print(f"\nìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        print("\n" + "=" * 60)
        print("ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("API í˜¸ì¶œ ìš”ì•½:")
        print(f"- Gemini: {api_call_counter['gemini']}íšŒ")
        print(f"- Notion: {api_call_counter['notion']}íšŒ")
        print(f"ì´ í˜¸ì¶œ: {sum(api_call_counter.values())}íšŒ")
        print("=" * 60)

if __name__ == "__main__":
    main()
